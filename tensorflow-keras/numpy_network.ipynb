{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Matrix Error 0.004020015897052594\n",
      "Bias error 0.000957033580579747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Parameter():\n",
    "  def __init__(self, tensor):\n",
    "    self.tensor = tensor\n",
    "    self.gradient = np.zeros_like(self.tensor)\n",
    "\n",
    "class Layer:\n",
    "  def __init__(self):\n",
    "    self.parameters = []\n",
    "\n",
    "  def forward(self, X):\n",
    "    return X, lambda D: D\n",
    "\n",
    "  def build_param(self, tensor):\n",
    "    param = Parameter(tensor)\n",
    "    self.parameters.append(param)\n",
    "    return param\n",
    "\n",
    "  def update(self, optimizer):\n",
    "    for param in self.parameters: optimizer.update(param)\n",
    "\n",
    "class Linear(Layer):\n",
    "  def __init__(self, inputs, outputs):\n",
    "    super().__init__()\n",
    "    self.weights = self.build_param(np.random.randn(inputs, outputs) * np.sqrt(1 / inputs))\n",
    "    self.bias = self.build_param(np.zeros(outputs))\n",
    "    \n",
    "  def forward(self, X):\n",
    "    def backward(D):\n",
    "      self.weights.gradient += X.T @ D\n",
    "      self.bias.gradient += D.sum(axis=0)\n",
    "      return D @ self.weights.tensor.T\n",
    "    return X @ self.weights.tensor + self.bias.tensor, backward\n",
    "  \n",
    "class Sequential(Layer):\n",
    "  def __init__(self, *layers):\n",
    "    super().__init__()\n",
    "    self.layers = layers\n",
    "    for layer in layers:\n",
    "      self.parameters.extend(layer.parameters)\n",
    "    \n",
    "  def forward(self, X):\n",
    "    backprops = []\n",
    "    Y = X\n",
    "    for layer in self.layers:\n",
    "      Y, backprop = layer.forward(Y)\n",
    "      backprops.append(backprop)\n",
    "    def backward(D):\n",
    "      for backprop in reversed(backprops):\n",
    "        D = backprop(D)\n",
    "      return D\n",
    "    return Y, backward\n",
    "\n",
    "class ReLu(Layer):\n",
    "  def forward(self, X):\n",
    "    mask = X > 0\n",
    "    return X * mask, lambda D: D * mask\n",
    "  \n",
    "class Sigmoid(Layer):\n",
    "  def forward(self, X):\n",
    "    S = 1 / (1 + np.exp(-X))\n",
    "    def backward(D):\n",
    "      return D * S * (1 - S)\n",
    "    return S, backward\n",
    "\n",
    "def mse_loss(Y_, Y):\n",
    "  diff = Y_ - Y.reshape(Y_.shape)\n",
    "  return np.square(diff).mean(), 2 * diff / len(diff)\n",
    "  \n",
    "def ce_loss(Y_, Y):\n",
    "  num = np.exp(Y_)\n",
    "  den = num.sum(axis=1).reshape(-1, 1)\n",
    "  prob = num / den\n",
    "  log_den = np.log(den)\n",
    "  ce = np.inner(Y_ - log_den, Y)\n",
    "  return ce.mean(), Y - prob / len(Y)\n",
    "\n",
    "class SGDOptimizer():\n",
    "  def __init__(self, lr=0.1):\n",
    "    self.lr = lr\n",
    "\n",
    "  def update(self, param):\n",
    "    param.tensor -= self.lr * param.gradient\n",
    "    param.gradient.fill(0)\n",
    "    \n",
    "class Learner():\n",
    "  def __init__(self, model, loss, optimizer):\n",
    "    self.model = model\n",
    "    self.loss = loss\n",
    "    self.optimizer = optimizer\n",
    "      \n",
    "  def fit_batch(self, X, Y):\n",
    "    Y_, backward = self.model.forward(X)\n",
    "    L, D = self.loss(Y_, Y)\n",
    "    backward(D)\n",
    "    self.model.update(self.optimizer)\n",
    "    return L\n",
    "    \n",
    "  def fit(self, X, Y, epochs, bs):\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "      p = np.random.permutation(len(X))\n",
    "      L = 0\n",
    "      for i in range(0, len(X), bs):\n",
    "        X_batch = X[p[i:i + bs]]\n",
    "        Y_batch = Y[p[i:i + bs]]\n",
    "        L += self.fit_batch(X_batch, Y_batch)\n",
    "      losses.append(L)\n",
    "    return losses\n",
    "\n",
    "#@title A simple linear dataset\n",
    "num_features = 10 #@param {type:\"slider\", min:5, max:100, step:1}\n",
    "num_samples = 100 #@param {type:\"slider\", min:10, max:1000, step:1}\n",
    "epochs = 10 #@param {type:\"slider\", min:1, max:100, step:1}\n",
    "batch_size = 10 #@param {type:\"slider\", min:1, max:100, step:1}\n",
    "learning_rate = 0.05 #@param {type:\"slider\", min:0.001, max:1.0, step:0.001}\n",
    "\n",
    "m = Linear(num_features, 1)\n",
    "model = Sequential(m)\n",
    "l = Learner(model, mse_loss, SGDOptimizer(lr=learning_rate))\n",
    "X = np.random.randn(num_samples, num_features)\n",
    "W = np.random.randn(num_features, 1)\n",
    "B = np.random.randn(1)\n",
    "Y = X @ W + B + 0.01 * np.random.randn(num_samples, 1)\n",
    "plt.plot(l.fit(X, Y, epochs=epochs, bs=batch_size))\n",
    "print('Weight Matrix Error', np.linalg.norm(m.weights.tensor - W))\n",
    "print('Bias error', np.abs(m.bias.tensor - B)[0])\n",
    "\n",
    "#@title A non-linear dataset\n",
    "num_samples = 1000 #@param {type:\"slider\", min:100, max:10000, step:1}\n",
    "epochs = 50 #@param {type:\"slider\", min:1, max:200, step:1}\n",
    "\n",
    "one_layer_batch_size = 50 #@param {type:\"slider\", min:1, max:100, step:1}\n",
    "one_layer_learning_rate = 0.01 #@param {type:\"slider\", min:0.001, max:1.0, step:0.001}\n",
    "\n",
    "two_layer_batch_size = 50 #@param {type:\"slider\", min:1, max:100, step:1}\n",
    "two_layer_learning_rate = 0.3 #@param {type:\"slider\", min:0.001, max:1.0, step:0.001}\n",
    "hidden_neurons = 10 #@param {type:\"slider\", min:1, max:200, step:1}\n",
    "\n",
    "X = np.random.randn(num_samples, 2)\n",
    "Y = X[:, 0] * X[:, 1]\n",
    "\n",
    "losses1 = Learner(\n",
    "    Sequential(Linear(2, 1)), \n",
    "    mse_loss, \n",
    "    SGDOptimizer(lr=one_layer_learning_rate)\n",
    ").fit(X, Y, epochs=epochs, bs=one_layer_batch_size)\n",
    "\n",
    "losses2 = Learner(\n",
    "    Sequential(\n",
    "        Linear(2, hidden_neurons), \n",
    "        Sigmoid(), \n",
    "        Linear(hidden_neurons, 1)\n",
    "    ), \n",
    "    mse_loss, \n",
    "    SGDOptimizer(lr=two_layer_learning_rate)\n",
    ").fit(X, Y, epochs=epochs, bs=two_layer_batch_size)\n",
    "\n",
    "plt.plot(losses1)\n",
    "plt.plot(losses2)\n",
    "plt.legend(['1 Layer', '2 Layers'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
