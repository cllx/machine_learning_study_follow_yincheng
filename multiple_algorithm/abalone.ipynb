{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.021715747908082728 -0.020459454600550542 -0.021145317303156386\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import isotonic\n",
    "from sklearn import cross_decomposition\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "data = pd.read_csv(r'./abalone.data', header=None)\n",
    "x = data.iloc[:, 1:]\n",
    "y = data[0]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,train_size=0.95,random_state=random.randint(1,100))\n",
    "mydict = {'F':1, 'M':-1, 'I':0}\n",
    "y_train = y_train.map(mydict) #把非字符型数据映射为字符型数据\n",
    "y_test = y_test.map(mydict)\n",
    "lr = LinearRegression()\n",
    "score1 = lr.fit(x_train, y_train).score(x_test, y_test)\n",
    "ridge = Ridge()\n",
    "score2 = ridge.fit(x_train, y_train).score(x_test, y_test)\n",
    "lasso = Lasso()\n",
    "score3 = lasso.fit(x_train, y_train).score(x_test, y_test)\n",
    "print(score1, score2, score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line -0.021715747908082728\n",
      "log 0.49760765550239233\n",
      "bayes 0.3923444976076555\n",
      "knn 0.45454545454545453\n",
      "forest 0.5263157894736842\n",
      "dtree 0.45933014354066987\n",
      "gbdt 0.5167464114832536\n",
      "svm 0.4688995215311005\n",
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svmcr 0.48325358851674644\n",
      "mlp 0.46411483253588515\n",
      "mlppreg 0.003399158943769187\n",
      "Ridge -0.020459454600550542\n",
      "Lasso -0.021145317303156386\n",
      "bayeslinear -0.019640242967670973\n",
      "SGDRegressor -0.37620102012333256\n",
      "PassiveAggressiveReg -4.264798443705215\n",
      "Perceptron 0.45454545454545453\n",
      "Lars -0.02171574790808295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNetCV -0.02067343933164345\n",
      "LassoLars -0.021145317303156386\n",
      "ARDRegression -0.012693346930868987\n",
      "RANSACRegressor -0.5960142549706309\n",
      "TheilSenRegressor -0.04392400540342756\n",
      "SGDClassifier 0.5550239234449761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianProcessRegressor -818.107862595028\n",
      "PLSRegression -0.014570311633106403\n",
      "PLSCanonical -4.953935752616433\n",
      "BernoulliNB 0.291866028708134\n",
      "GaussianNB 0.5358851674641149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\cross_decomposition\\pls_.py:287: UserWarning: Y residual constant at iteration 1\n",
      "  warnings.warn('Y residual constant at iteration %s' % k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier 0.4880382775119617\n",
      "EXtraTreeClassifier 0.5358851674641149\n",
      "AdaBoostClassifier 0.46411483253588515\n",
      "GradientBoostingClassifier 0.5358851674641149\n",
      "....max  : ['SGDClassifier'] 0.5550239234449761\n",
      "....nextmax  : ['GradientBoostingClassifier', 'SGDClassifier']\n",
      "....nextmax  : GradientBoostingClassifier 0.5358851674641149\n",
      "....thirdmax  : EXtraTreeClassifier 0.5358851674641149\n"
     ]
    }
   ],
   "source": [
    "# 线性回归算法，最小二乘法，函数名 LinearRegression\n",
    "def mx_line(train_x, train_y):\n",
    "    mx = LinearRegression()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 逻辑回归算法，函数名, LogisticRegression\n",
    "def mx_log(train_x, train_y):\n",
    "#     mx = LogisticRegression(penalty='12')\n",
    "    mx = LogisticRegression()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 多项式朴素贝叶斯算法，Multinomial Naive Bayes，函数名，multinomialnb\n",
    "def mx_bayes(train_x, train_y):\n",
    "    mx = MultinomialNB(alpha=0.01)\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "mx = mx_bayes(x_train, y_train)\n",
    "mx.score(x_test, y_test)\n",
    "\n",
    "# KNN近邻算法，函数名，KNeighborsClassifier\n",
    "def mx_knn(train_x, train_y):\n",
    "    mx = KNeighborsClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 随机随机森林算法,random forest classifier，函数名,RandomForestClassifier\n",
    "def mx_forest(train_x, train_y):\n",
    "    mx = RandomForestClassifier(n_estimators=8)\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 决策树方法\n",
    "def mx_dtree(train_x, train_y):\n",
    "    mx = tree.DecisionTreeClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# GBDT迭代决策树算法，Gradient Boosting Decision Tree,\n",
    "# 又叫MART(Multiple Additive Regression Tree)，函数名，GradientBoostingClassifier\n",
    "def mx_GBDT(train_x, train_y):\n",
    "    mx = GradientBoostingClassifier(n_estimators=200)\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# SVM向量机算法，函数名 SVC\n",
    "def mx_svm(train_x, train_y):\n",
    "    mx = SVC(kernel='rbf', probability=True)\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# SVM-cross向量机交叉算法，函数名 SVC，自动调优\n",
    "def mx_svm_cross(train_x, train_y):\n",
    "    mx = SVC(kernel='rbf', probability=True)\n",
    "    param_grid = {'C':[1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma':[0.001, 0.0001]}\n",
    "    grid_search = GridSearchCV(mx, param_grid, n_jobs=1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    mx = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# MLP神经网络算法\n",
    "def mx_MLP(train_x, train_y):\n",
    "    # mx = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=1)\n",
    "    mx = MLPClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 神经网络回归算法\n",
    "def mx_MLP_reg(train_x, train_y):\n",
    "    # mx = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=1)\n",
    "    mx = MLPRegressor()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 岭回归算法\n",
    "def mx_Ridge(train_x, train_y):\n",
    "    mx = linear_model.Ridge()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# lasso回归(拉格朗日回归)\n",
    "def mx_Lasso(train_x, train_y):\n",
    "    mx = linear_model.Lasso()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 贝叶斯回归\n",
    "def mx_bayeslinear(train_x, train_y):\n",
    "    mx = linear_model.BayesianRidge()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 随机梯度\n",
    "def mx_SGDRegressor(train_x, train_y):\n",
    "    mx = linear_model.SGDRegressor()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 被动攻击算法\n",
    "def mx_PassiveAggressiveReg(train_x, train_y):\n",
    "    mx = linear_model.PassiveAggressiveRegressor()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 感知器算法\n",
    "def mx_Perceptron(train_x, train_y):\n",
    "    mx = linear_model.Perceptron()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 最小角回归\n",
    "def mx_Lars(train_x, train_y):\n",
    "    mx = linear_model.Lars()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 弹性网络\n",
    "def mx_ElasticNetCV(train_x, train_y):\n",
    "    mx = linear_model.ElasticNetCV()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 多任务拉格朗日回归\n",
    "def mx_LassoLars(train_x, train_y):\n",
    "    mx = linear_model.LassoLars()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 主动决策\n",
    "def mx_ARDRegression(train_x, train_y):\n",
    "    mx = linear_model.ARDRegression()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 随机抽样一致性算法\n",
    "def mx_RANSACRegressor(train_x, train_y):\n",
    "    mx = linear_model.RANSACRegressor()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 广义中值估计\n",
    "def mx_TheilSenRegressor(train_x, train_y):\n",
    "    mx = linear_model.TheilSenRegressor()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 随机梯度下降\n",
    "def mx_SGDClassifier(train_x, train_y):\n",
    "    mx = SGDClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 高斯过程分类器\n",
    "def mx_GaussianProcessRegressor(train_x, train_y):\n",
    "    mx = GaussianProcessRegressor()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 交叉分解\n",
    "def mx_PLSRegression(train_x, train_y):\n",
    "    mx = sklearn.cross_decomposition.PLSRegression()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "def mx_PLSCanonical(train_x, train_y):\n",
    "    mx = cross_decomposition.PLSCanonical()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 伯努利贝叶斯\n",
    "def mx_BernoulliNB(train_x, train_y):\n",
    "    mx = sklearn.naive_bayes.BernoulliNB()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 高斯贝叶斯\n",
    "def mx_GaussianNB(train_x, train_y):\n",
    "    mx = sklearn.naive_bayes.GaussianNB()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 元估计器\n",
    "def mx_BaggingClassifier(train_x, train_y):\n",
    "    mx = BaggingClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 极限组合森林\n",
    "def mx_EXtraTreeClassifier(train_x, train_y):\n",
    "    mx = ExtraTreesClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# AdaBoost(提升)算法\n",
    "def mx_AdaBoostClassifier(train_x, train_y):\n",
    "    mx = AdaBoostClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 梯度树算法GradientBoostingClassifier\n",
    "def mx_GradientBoostingClassifier(train_x, train_y):\n",
    "    mx = GradientBoostingClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "mxfunSgn = {'line': mx_line,\n",
    "            'log': mx_log,\n",
    "            'bayes': mx_bayes,\n",
    "            'knn': mx_knn,\n",
    "            'forest': mx_forest,\n",
    "            'dtree': mx_dtree,\n",
    "            'gbdt': mx_GBDT,\n",
    "            'svm': mx_svm,\n",
    "            'svmcr': mx_svm_cross,\n",
    "            'mlp': mx_MLP,\n",
    "            'mlppreg': mx_MLP_reg,\n",
    "            'Ridge': mx_Ridge,\n",
    "            'Lasso': mx_Lasso,\n",
    "            'bayeslinear': mx_bayeslinear,\n",
    "            'SGDRegressor': mx_SGDRegressor,\n",
    "            'PassiveAggressiveReg': mx_PassiveAggressiveReg,\n",
    "            'Perceptron': mx_Perceptron,\n",
    "            'Lars': mx_Lars,\n",
    "            'ElasticNetCV': mx_ElasticNetCV,\n",
    "            'LassoLars': mx_LassoLars,\n",
    "            'ARDRegression': mx_ARDRegression,\n",
    "            'RANSACRegressor': mx_RANSACRegressor,\n",
    "            'TheilSenRegressor': mx_TheilSenRegressor,\n",
    "            'SGDClassifier': mx_SGDClassifier,\n",
    "            'GaussianProcessRegressor': mx_GaussianProcessRegressor,\n",
    "            'PLSRegression': mx_PLSRegression,\n",
    "            'PLSCanonical': mx_PLSCanonical,\n",
    "            'BernoulliNB': mx_BernoulliNB,\n",
    "            'GaussianNB': mx_GaussianNB,\n",
    "            'BaggingClassifier': mx_BaggingClassifier,\n",
    "            'EXtraTreeClassifier': mx_EXtraTreeClassifier,\n",
    "            'AdaBoostClassifier': mx_AdaBoostClassifier,\n",
    "            'GradientBoostingClassifier': mx_GradientBoostingClassifier,\n",
    "            }\n",
    "\n",
    "resultlist = {}\n",
    "for k in mxfunSgn:\n",
    "    mx = mxfunSgn[k](x_train, y_train)\n",
    "    score = mx.score(x_test, y_test)\n",
    "    print(k, score)\n",
    "    resultlist[k] = score\n",
    "    \n",
    "print(\"....max  :\", sorted(resultlist, key=lambda x:resultlist[x])[-1:],\n",
    "      resultlist[sorted(resultlist, key=lambda x:resultlist[x])[-1]])\n",
    "print(\"....nextmax  :\", sorted(resultlist, key=lambda x:resultlist[x])[-2:])\n",
    "print(\"....nextmax  :\", sorted(resultlist, key=lambda x:resultlist[x])[-2],\n",
    "      resultlist[sorted(resultlist, key=lambda x:resultlist[x])[-2]])\n",
    "print(\"....thirdmax  :\", sorted(resultlist, key=lambda x:resultlist[x])[-3],\n",
    "      resultlist[sorted(resultlist, key=lambda x:resultlist[x])[-3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
