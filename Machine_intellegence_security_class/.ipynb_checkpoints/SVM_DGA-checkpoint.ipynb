{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载预先生成的特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      (10000, 755)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":     \n",
    "    #######################train########################   \n",
    "    fp=open(\"feature_data.pkl\",'rb')\n",
    "    feature_data=pickle.load(fp, encoding='bytes')\n",
    "    print(\"     \",feature_data.shape)\n",
    "    fp.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](SVM_DGA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载样本的真实标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      (10000,)\n"
     ]
    }
   ],
   "source": [
    "    label_file = open('y.txt','r')\n",
    "    label_data=[]\n",
    "    for line in label_file:\n",
    "        label = int(line.replace('\\n','').replace('\\r\\n',''))\n",
    "        label_data.append(label)\n",
    "    \n",
    "    label_data = np.array(label_data) \n",
    "    print(\"     \",label_data.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分测试集和训练集，训练集固定为后1000个样本点，测试集可以通过cut_off调整比例个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       (9000, 755)\n",
      "       (1000, 755)\n",
      "       (9000,)\n",
      "       (1000,)\n"
     ]
    }
   ],
   "source": [
    "    import random\n",
    "    #random initialization\n",
    "    n_samples, n_features = feature_data.shape\n",
    "    p = list(range(n_samples))  # Shuffle samples    \n",
    "    random.seed(12345)    \n",
    "    random.shuffle(p)\n",
    "    XX,yy = feature_data[p],label_data[p]\n",
    "    cut_off = int(n_samples / 10)*1\n",
    "    \n",
    "    train_feature=XX[:9000]\n",
    "    train_label=yy[:9000]\n",
    "    test_feature=XX[9000:n_samples]\n",
    "    test_label=yy[9000:n_samples]\n",
    "    print(\"      \",train_feature.shape)\n",
    "    print(\"      \",test_feature.shape)\n",
    "    print(\"      \",train_label.shape)\n",
    "    print(\"      \",test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import isotonic\n",
    "from sklearn import cross_decomposition\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型，选择模型及调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...... training and save model\n",
      "     SVM_time : 13.286 s\n"
     ]
    }
   ],
   "source": [
    "    print('...... training and save model')\n",
    "    t0 = time.time()\n",
    "    SVC1 = SVC(kernel='linear',C=1)\n",
    "    SVC2 = GridSearchCV(SVC(kernel='linear'), n_jobs=4,\n",
    "                              param_grid={\"C\": np.logspace(-1, 1, 7)})    \n",
    "    SVC3 = SVC(kernel='rbf',C=1,gamma=1)#10,0.1\n",
    "    SVC4 = GridSearchCV(SVC(kernel='rbf'), n_jobs=4,\n",
    "                           param_grid={\"C\": np.logspace(0, 10, 6),\n",
    "                                       \"gamma\":np.logspace(-10,-0, 6)})  \n",
    "    \n",
    "    SVC1.fit(train_feature, train_label)  \n",
    "    #print 'best_param:',classifier.best_params_\n",
    "    \n",
    "    fp=open(\"SVM_train_model.pkl\",\"wb\")\n",
    "    pickle.dump(SVC1,fp,-1)\n",
    "    fp.close()  \n",
    "    SVM_time = time.time() - t0\n",
    "    print(\"     SVM_time : %.3f s\" % SVM_time)#    \n",
    "    \n",
    "    t0 = time.time()\n",
    "    fp=open(\"SVM_train_model.pkl\",'rb')\n",
    "    classifier=pickle.load(fp)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [1 1 0 1 0 0 1 1 1 1]\n",
      "       SVC1 0.978\n",
      "      [1 1 0 1 0 0 1 1 1 1]\n",
      "       SVC2 0.976\n",
      "      [0 1 0 1 0 0 1 1 1 1]\n",
      "       SVC3 0.969\n",
      "      [ 0.77985382  1.0369339  -0.05973053  1.07426453  0.02319336  0.25017548\n",
      "  1.04198456  0.98636627  1.08132172  0.80886841]\n",
      "       line 0.0\n",
      "      [1 1 0 1 0 0 1 1 1 1]\n",
      "       log 0.969\n",
      "      [1 1 0 1 0 0 1 1 1 1]\n",
      "       bayes 0.975\n",
      "      [1 1 0 1 0 0 1 1 1 1]\n",
      "       knn 0.964\n",
      "      [1 1 0 1 0 0 1 1 1 1]\n",
      "       forest 0.969\n",
      "      [1 1 0 1 0 0 1 1 1 1]\n",
      "       dtree 0.965\n",
      "      [1 1 0 1 0 0 1 1 1 1]\n",
      "       gbdt 0.975\n",
      "      [1 1 0 1 0 0 1 1 1 1]\n",
      "       svm 0.898\n",
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed: 83.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [1 1 0 1 0 0 1 1 1 1]\n",
      "       svmcr 0.976\n",
      "      [1 1 0 1 0 0 1 1 1 1]\n",
      "       mlp 0.981\n",
      "      [0.98418351 0.96043294 0.01634114 0.97623324 0.00134633 0.02711276\n",
      " 1.0012775  0.96804171 0.9738886  0.98665895]\n",
      "       mlppreg 0.0\n",
      "      [ 0.78390171  1.0379362  -0.06013233  1.06133699  0.02281385  0.23858893\n",
      "  1.03271154  0.97156559  1.07998373  0.81248029]\n",
      "       Ridge 0.0\n",
      "      [0.60355556 0.60355556 0.60355556 0.60355556 0.60355556 0.60355556\n",
      " 0.60355556 0.60355556 0.60355556 0.60355556]\n",
      "       Lasso 0.0\n",
      "      [ 0.78160576  1.03785777 -0.05951001  1.06883437  0.02271315  0.24574318\n",
      "  1.03840305  0.98047973  1.0808038   0.81001909]\n",
      "       bayeslinear 0.0\n",
      "      [ 0.88551701  1.00956255 -0.10810786  0.97569792  0.0195338   0.30516112\n",
      "  0.94721814  0.89261721  1.05724673  0.89054617]\n",
      "       SGDRegressor 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [ 0.96015339  1.17419757 -0.05202012  1.21525556  0.09075973  0.70163989\n",
      "  1.21424812  1.19524307  1.23694363  1.00879509]\n",
      "       PassiveAggressiveReg 0.0\n",
      "      [1 1 0 1 0 0 1 1 1 1]\n",
      "       Perceptron 0.941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [ 0.78194427  1.03907013 -0.06236267  1.0666275   0.01927185  0.25350189\n",
      "  1.04061127  0.98327637  1.08383179  0.81162262]\n",
      "       Lars 0.0\n",
      "      [ 0.83722467  1.02842486 -0.06796285  1.03061967  0.02664473  0.22566562\n",
      "  1.00017177  0.93142218  1.07440615  0.86546121]\n",
      "       ElasticNetCV 0.0\n",
      "      [0.60355556 0.60355556 0.60355556 0.60355556 0.60355556 0.60355556\n",
      " 0.60355556 0.60355556 0.60355556 0.60355556]\n",
      "       LassoLars 0.0\n"
     ]
    }
   ],
   "source": [
    "# 不同参数的svm算法\n",
    "def SVC1(train_feature, train_label):\n",
    "    SVC1 = SVC(kernel='linear',C=1)\n",
    "    SVC1.fit(train_feature, train_label)\n",
    "    return SVC1\n",
    "\n",
    "def SVC2(train_feature, train_label):\n",
    "    SVC2 = GridSearchCV(SVC(kernel='linear'), n_jobs=4,\n",
    "                              param_grid={\"C\": np.logspace(-1, 1, 7)})\n",
    "    SVC2.fit(train_feature, train_label)\n",
    "    return SVC2\n",
    "\n",
    "def SVC3(train_feature, train_label):\n",
    "    SVC3 = SVC(kernel='rbf',C=1,gamma=1)#10,0.1\n",
    "    SVC3.fit(train_feature, train_label)\n",
    "    return SVC3\n",
    "\n",
    "def SVC4(train_feature, train_label):\n",
    "    SVC4 = GridSearchCV(SVC(kernel='rbf'), n_jobs=4,\n",
    "                           param_grid={\"C\": np.logspace(0, 10, 6),\n",
    "                                       \"gamma\":np.logspace(-10,-0, 6)})\n",
    "    SVC4.fit(train_feature, train_label)\n",
    "    return SVC4\n",
    "\n",
    "# 线性回归算法，最小二乘法，函数名 LinearRegression\n",
    "def mx_line(train_x, train_y):\n",
    "    mx = LinearRegression()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 逻辑回归算法，函数名, LogisticRegression\n",
    "def mx_log(train_x, train_y):\n",
    "#     mx = LogisticRegression(penalty='12')\n",
    "    mx = LogisticRegression()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 多项式朴素贝叶斯算法，Multinomial Naive Bayes，函数名，multinomialnb\n",
    "def mx_bayes(train_x, train_y):\n",
    "    mx = MultinomialNB(alpha=0.01)\n",
    "    mx.fit(train_x, train_y.astype('int'))\n",
    "    return mx\n",
    "\n",
    "# KNN近邻算法，函数名，KNeighborsClassifier\n",
    "def mx_knn(train_x, train_y):\n",
    "    mx = KNeighborsClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 随机随机森林算法,random forest classifier，函数名,RandomForestClassifier\n",
    "def mx_forest(train_x, train_y):\n",
    "    mx = RandomForestClassifier(n_estimators=8)\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 决策树方法\n",
    "def mx_dtree(train_x, train_y):\n",
    "    mx = tree.DecisionTreeClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# GBDT迭代决策树算法，Gradient Boosting Decision Tree,\n",
    "# 又叫MART(Multiple Additive Regression Tree)，函数名，GradientBoostingClassifier\n",
    "def mx_GBDT(train_x, train_y):\n",
    "    mx = GradientBoostingClassifier(n_estimators=200)\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# SVM向量机算法，函数名 SVC\n",
    "def mx_svm(train_x, train_y):\n",
    "    mx = SVC(kernel='rbf', probability=True)\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# SVM-cross向量机交叉算法，函数名 SVC，自动调优\n",
    "def mx_svm_cross(train_x, train_y):\n",
    "    mx = SVC(kernel='rbf', probability=True)\n",
    "    param_grid = {'C':[1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma':[0.001, 0.0001]}\n",
    "    grid_search = GridSearchCV(mx, param_grid, n_jobs=1, verbose=1)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    mx = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# MLP神经网络算法\n",
    "def mx_MLP(train_x, train_y):\n",
    "    # mx = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=1)\n",
    "    mx = MLPClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 神经网络回归算法\n",
    "def mx_MLP_reg(train_x, train_y):\n",
    "    # mx = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=1)\n",
    "    mx = MLPRegressor()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 岭回归算法\n",
    "def mx_Ridge(train_x, train_y):\n",
    "    mx = linear_model.Ridge()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# lasso回归(拉格朗日回归)\n",
    "def mx_Lasso(train_x, train_y):\n",
    "    mx = linear_model.Lasso()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 贝叶斯回归\n",
    "def mx_bayeslinear(train_x, train_y):\n",
    "    mx = linear_model.BayesianRidge()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 随机梯度\n",
    "def mx_SGDRegressor(train_x, train_y):\n",
    "    mx = linear_model.SGDRegressor()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 被动攻击算法\n",
    "def mx_PassiveAggressiveReg(train_x, train_y):\n",
    "    mx = linear_model.PassiveAggressiveRegressor()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 感知器算法\n",
    "def mx_Perceptron(train_x, train_y):\n",
    "    mx = linear_model.Perceptron()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 最小角回归\n",
    "def mx_Lars(train_x, train_y):\n",
    "    mx = linear_model.Lars()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 弹性网络\n",
    "def mx_ElasticNetCV(train_x, train_y):\n",
    "    mx = linear_model.ElasticNetCV()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 多任务拉格朗日回归\n",
    "def mx_LassoLars(train_x, train_y):\n",
    "    mx = linear_model.LassoLars()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 主动决策\n",
    "def mx_ARDRegression(train_x, train_y):\n",
    "    mx = linear_model.ARDRegression()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 随机抽样一致性算法\n",
    "def mx_RANSACRegressor(train_x, train_y):\n",
    "    mx = linear_model.RANSACRegressor()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 广义中值估计\n",
    "def mx_TheilSenRegressor(train_x, train_y):\n",
    "    mx = linear_model.TheilSenRegressor()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 随机梯度下降\n",
    "def mx_SGDClassifier(train_x, train_y):\n",
    "    mx = SGDClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 高斯过程分类器\n",
    "def mx_GaussianProcessRegressor(train_x, train_y):\n",
    "    mx = GaussianProcessRegressor()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 交叉分解\n",
    "def mx_PLSRegression(train_x, train_y):\n",
    "    mx = sklearn.cross_decomposition.PLSRegression()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "def mx_PLSCanonical(train_x, train_y):\n",
    "    mx = cross_decomposition.PLSCanonical()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 伯努利贝叶斯\n",
    "def mx_BernoulliNB(train_x, train_y):\n",
    "    mx = sklearn.naive_bayes.BernoulliNB()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 高斯贝叶斯\n",
    "def mx_GaussianNB(train_x, train_y):\n",
    "    mx = sklearn.naive_bayes.GaussianNB()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 元估计器\n",
    "def mx_BaggingClassifier(train_x, train_y):\n",
    "    mx = BaggingClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 极限组合森林\n",
    "def mx_EXtraTreeClassifier(train_x, train_y):\n",
    "    mx = ExtraTreesClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# AdaBoost(提升)算法\n",
    "def mx_AdaBoostClassifier(train_x, train_y):\n",
    "    mx = AdaBoostClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "# 梯度树算法GradientBoostingClassifier\n",
    "def mx_GradientBoostingClassifier(train_x, train_y):\n",
    "    mx = GradientBoostingClassifier()\n",
    "    mx.fit(train_x, train_y)\n",
    "    return mx\n",
    "\n",
    "mxfunSgn = {'SVC1': SVC1,\n",
    "            'SVC2': SVC2,\n",
    "            'SVC3': SVC3,\n",
    "            'SVC3': SVC4,\n",
    "            'line': mx_line,\n",
    "            'log': mx_log,\n",
    "            'bayes': mx_bayes,\n",
    "            'knn': mx_knn,\n",
    "            'forest': mx_forest,\n",
    "            'dtree': mx_dtree,\n",
    "            'gbdt': mx_GBDT,\n",
    "            'svm': mx_svm,\n",
    "            'svmcr': mx_svm_cross,\n",
    "            'mlp': mx_MLP,\n",
    "            'mlppreg': mx_MLP_reg,\n",
    "            'Ridge': mx_Ridge,\n",
    "            'Lasso': mx_Lasso,\n",
    "            'bayeslinear': mx_bayeslinear,\n",
    "            'SGDRegressor': mx_SGDRegressor,\n",
    "            'PassiveAggressiveReg': mx_PassiveAggressiveReg,\n",
    "            'Perceptron': mx_Perceptron,\n",
    "            'Lars': mx_Lars,\n",
    "            'ElasticNetCV': mx_ElasticNetCV,\n",
    "            'LassoLars': mx_LassoLars,\n",
    "            'ARDRegression': mx_ARDRegression,\n",
    "            'RANSACRegressor': mx_RANSACRegressor,\n",
    "            'TheilSenRegressor': mx_TheilSenRegressor,\n",
    "            'SGDClassifier': mx_SGDClassifier,\n",
    "            'GaussianProcessRegressor': mx_GaussianProcessRegressor,\n",
    "            'PLSRegression': mx_PLSRegression,\n",
    "            'PLSCanonical': mx_PLSCanonical,\n",
    "            'BernoulliNB': mx_BernoulliNB,\n",
    "            'GaussianNB': mx_GaussianNB,\n",
    "            'BaggingClassifier': mx_BaggingClassifier,\n",
    "            'EXtraTreeClassifier': mx_EXtraTreeClassifier,\n",
    "            'AdaBoostClassifier': mx_AdaBoostClassifier,\n",
    "            'GradientBoostingClassifier': mx_GradientBoostingClassifier,\n",
    "            }\n",
    "\n",
    "resultlist = {}\n",
    "for k in mxfunSgn:\n",
    "    try:\n",
    "        mx = mxfunSgn[k](train_feature, train_label)\n",
    "        score = mx.predict(test_feature)\n",
    "        count=0\n",
    "        for i in range(test_label.shape[0]):\n",
    "            if score[i]==test_label[i]:\n",
    "                count+=1\n",
    "        accuracy=1.0*count/test_label.shape[0]\n",
    "        print(\"     \",score[:10])\n",
    "        print(\"      \",k, accuracy)\n",
    "        resultlist[k] = accuracy\n",
    "    except:\n",
    "        score = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算最终的准确率指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [1 1 0 1 0 0 1 1 1 1]\n",
      "     predict time: 13.758301258087158\n",
      "     accuracy:  0.978\n"
     ]
    }
   ],
   "source": [
    "    probas_ = classifier.predict(test_feature)\n",
    "    print(\"     \",probas_[:10])\n",
    "     \n",
    "    print('     predict time:',time.time() - t0)\n",
    "    count=0\n",
    "    for i in range(test_label.shape[0]):\n",
    "        if probas_[i]==test_label[i]:\n",
    "            count+=1\n",
    "            \n",
    "    accuracy=1.0*count/test_label.shape[0]\n",
    "    print(\"     accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"....max  :\", sorted(resultlist, key=lambda x:resultlist[x])[-1:],\n",
    "      resultlist[sorted(resultlist, key=lambda x:resultlist[x])[-1]])\n",
    "print(\"....nextmax  :\", sorted(resultlist, key=lambda x:resultlist[x])[-2],\n",
    "      resultlist[sorted(resultlist, key=lambda x:resultlist[x])[-2]])\n",
    "print(\"....thirdmax  :\", sorted(resultlist, key=lambda x:resultlist[x])[-3],\n",
    "      resultlist[sorted(resultlist, key=lambda x:resultlist[x])[-3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
